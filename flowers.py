# -*- coding: utf-8 -*-
"""Flowers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/WDoughty/aab5233ca54c34a2f0d499620e7edb5b/flowers.ipynb
"""

import tensorflow_datasets as tfds
import tensorflow as tf
from tensorflow import keras
from keras import layers
import numpy as np
import matplotlib.pyplot as plt

(train_ds, val_ds, test_ds), metadata = tfds.load(
    'tf_flowers',
    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
    with_info=True,
    as_supervised=True,
)

num_classes = metadata.features['label'].num_classes
print(num_classes)

get_label_name = metadata.features['label'].int2str

image, label = next(iter(train_ds))
_ = plt.imshow(image)
_ = plt.title(get_label_name(label))

IMG_SIZE = 180

resize_and_rescale = tf.keras.Sequential([
  layers.experimental.preprocessing.Resizing(IMG_SIZE, IMG_SIZE),
  layers.experimental.preprocessing.Rescaling(1./255)
])

result = resize_and_rescale(image)
_ = plt.imshow(result)

data_augmentation = tf.keras.Sequential([
  layers.experimental.preprocessing.RandomFlip("horizontal_and_vertical"),
  layers.experimental.preprocessing.RandomRotation(0.2),
])

image = tf.expand_dims(image, 0)

plt.figure(figsize=(10, 10))
for i in range(9):
  augmented_image = data_augmentation(image)
  ax = plt.subplot(3, 3, i + 1)
  plt.imshow(augmented_image[0])
  plt.axis("off")

aug_ds = train_ds.map(
  lambda x, y: (resize_and_rescale(x, training=True), y))

batch_size = 32
AUTOTUNE = tf.data.experimental.AUTOTUNE

def prepare(ds, shuffle=False, augment=False):
  # Resize and rescale all datasets
  ds = ds.map(lambda x, y: (resize_and_rescale(x), y), 
              num_parallel_calls=AUTOTUNE)

  if shuffle:
    ds = ds.shuffle(1000)

  # Batch all datasets
  ds = ds.batch(batch_size)

  # Use data augmentation only on the training set
  if augment:
    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), 
                num_parallel_calls=AUTOTUNE)

  # Use buffered prefecting on all datasets
  return ds.prefetch(buffer_size=AUTOTUNE)

train_ds = prepare(train_ds, shuffle=True, augment=True)
val_ds = prepare(val_ds)
test_ds = prepare(test_ds)



model = tf.keras.Sequential([ 
  layers.Conv2D(32, (3,3), activation='relu'),
  layers.MaxPooling2D(pool_size=(2,2)),
  layers.Conv2D(64, (3,3), activation='relu'),
  layers.MaxPooling2D(pool_size=(2,2)),
  layers.Conv2D(64, (3,3), activation='relu'),
  layers.MaxPooling2D(pool_size=(2,2)),
  layers.Flatten(),
  layers.Dense(512, activation='relu'),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes)
])

model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),optimizer='adam',metrics='accuracy')

myCallbacks = [
               tf.keras.callbacks.EarlyStopping(),
              #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5',save_best_only=True)
]

epochs=1
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs,
  callbacks= myCallbacks
)

loss, acc = model.evaluate(test_ds)
print("Accuracy", acc)

val_loss = history.history['val_loss']
print(val_loss)

history.history

from keras.models import Sequential
from keras.layers import Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, Conv2D
import random

#Start of the Genetic Algorithm

monitor= tf.keras.callbacks.EarlyStopping(monitor='val_recall', mode='max', verbose=1, patience=2)
checkpointer=tf.keras.callbacks.ModelCheckpoint(filepath="3 Azure models/2 portion 200/1 one eigen/classification.hdf5",verbose=1,save_best_only=True)

GACallbacks = [monitor, checkpointer]
myCallbacks = [monitor]

num_of_epochs = 1

#DNA class holds the DNA of each class and will run the model for each class
class DNA:
  def __init__(self,num_of_filters):
    self.x_1 = num_of_filters 
    self.binary_string_x_1 = self.encode(self.x_1)
    self.x_1_DNA = [*self.binary_string_x_1]
    self.x_1_length = len(self.x_1_DNA)
    self.model = self.build_model()
    self.history = self.model.fit(
        test_ds,
        validation_data=val_ds,
        batch_size = 4,
        epochs=num_of_epochs,
        callbacks= myCallbacks
    )
    self.metrics = self.model.evaluate()
    self.fitness = self.metrics[0]
    #del self.model


  def build_model(self):
    self.m1 = tf.keras.Sequential([
      layers.Conv2D(self.x_1, (3,3), activation='relu'),
      layers.MaxPooling2D(pool_size=(2,2)),
      layers.Flatten(),
      layers.Dense(num_classes,activation = 'softmax')                       
    ])
    self.m1.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),optimizer='adam',metrics='accuracy')
    return self.m1

  def encode(self,n):
    return format(n,'09b') #9 bit padding for 0 to 511
  
  def decode(self, b):
    return int(b,2)
  
  def list_to_decimal(self):
    self.x_1 = self.decode(bin(int(''.join(map(str,self.x_1_DNA)), 2)))
  
  def update_child(self):
    self.list_to_decimal()
    self.binary_string_x_1 = self.encode(self.x_1)

#builds populations
class Population:
  def __init__(self, size):
    self.fittest = 0
    self.DNA_Array = []
    self.population_Size = size
    self.build_population(size)
  
  def build_population(self, size):
    for i in range(0, size):
      self.DNA_Array.append(DNA(random.randint(1,511)))
  
  def get_fittest(self):
    max_fit = self.DNA_Array[0].fitness
    max_fit_i = 0
    for i in range(0, len(self.DNA_Array)):
      if max_fit >= self.DNA_Array[i].fitness:
        max_fit = self.DNA_Array[i].fitness
        max_fit_i = i
    self.fittest = self.DNA_Array[max_fit_i].fitness
    return self.DNA_Array[max_fit_i]

#Drives the genetic algorithm

class Demo:

  def __init__(self,size):
    self.generation_count = 0
    self.population = Population(size)
    self.fittest = self.population.get_fittest()
    print("Generation: ", self.generation_count, " Fittest: ", self.fittest.fitness,  "  Number of filters = ", self.fittest.x_1)

    while self.generation_count < 101:
      new_pool = []
      self.generation_count+=1
      new_pool.clear()
      new_pool.append(DNA(self.fittest.x_1))
      for i in range(0, self.population.population_Size - 1):
        partner_a = random.randint(0, self.population.population_Size - 1)
        partner_b = random.randint(0, self.population.population_Size - 1)
        while partner_a == partner_b:
          partner_b = random.randint(0, self.population.population_Size - 1)
        child = self.crossover(self.population.DNA_Array[partner_a],  self.population.DNA_Array[partner_b])
        mutate_rate = random.randint(0,100)
        if mutate_rate < 5:
          child = self.mutate(child)
        new_pool.append(child)
      self.population.DNA_Array.clear()
      self.population.DNA_Array.extend(new_pool)
      self.fittest = self.population.get_fittest()
      print("Generation: ", self.generation_count, " Fittest: ", self.fittest.fitness,  "  Number of filters = ", self.fittest.x_1)

  def crossover(self, partner_a, partner_b):
    child = DNA(partner_a.x_1)
    child2 = DNA(partner_b.x_1)
    crossover_point = random.randint(1, partner_a.x_1_length - 2)
    crossover_point2 = random.randint(crossover_point, partner_a.x_1_length - 1)
    for i in range(crossover_point, crossover_point2):
      child.x_1_DNA[i] = partner_b.x_1_DNA[i]
      child2.x_1_DNA[i] = partner_a.x_1_DNA[i]
    child.update_child()
    child2.update_child()
    if child.x_1 == 0:
      child.x_1 = 1 
    if child2.x_1 ==0:
      child2.x_1 = 1
    c1 = DNA(child.x_1)
    c2 = DNA(child2.x_1)
    return c1 if c1.fitness < c2.fitness else c2

  def mutate(self, child):
    mutation_point = random.randint(0, child.x_1_length - 1)
    if child.x_1_DNA[mutation_point] == '0':
      child.x_1_DNA[mutation_point] = '1'
    else:
      child.x_1_DNA[mutation_point] = '0'
    child.update_child()
    if child.x_1 == 0:
      child.x_1 = 1
    c = DNA(child.x_1)
    return c

# Running out of memory for google colabs so i cant actually run completely

go = Demo(5)

















